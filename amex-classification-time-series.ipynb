{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"import os\nimport json\nimport numpy as np\nimport pandas as pd\nfrom pandas.io.json import json_normalize\nimport matplotlib.pyplot as plt\nimport seaborn as sns\ncolor = sns.color_palette()\n\nimport time\n\n%matplotlib inline\n\nfrom plotly import tools\nimport plotly.offline as py\npy.init_notebook_mode(connected=True)\nimport plotly.graph_objs as go\n\nfrom sklearn import model_selection, preprocessing, metrics\nimport lightgbm as lgb\n\nfrom sklearn.model_selection import KFold\nfrom tqdm import tqdm\nimport gc\nimport datetime\nimport re\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import matthews_corrcoef, roc_curve, auc, roc_auc_score, classification_report, confusion_matrix\nfrom sklearn.metrics import f1_score,precision_recall_curve,roc_curve, recall_score,precision_score\nfrom sklearn.model_selection import train_test_split\n\nfrom sklearn.model_selection import KFold,GroupKFold,StratifiedKFold\nfrom sklearn.metrics import mean_squared_error\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\npd.options.mode.chained_assignment = None\npd.options.display.max_columns = 999\npd.options.display.max_rows = 500","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"eb95d0b4f2b3a48e85718a855915e0c6ffac96fa"},"cell_type":"code","source":"# Path = '../input/amexml/'\nPath = '../input/amex-preproc/'\ntrain_df=pd.read_hdf(Path+'train_targetenc.hdf')\nprint('train reading complete')\ntest_df=pd.read_hdf(Path+'test_targetenc.hdf')\nprint('test reading complete')\nlog_df=pd.read_csv('../input/amexml/'+'historical_user_logs.csv')\nprint('log reading complete')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"10ba4613cc27a5463a00b4fd8be7bc0496126685"},"cell_type":"code","source":"Path = '../input/amexml/'\ntrain_rawdf=pd.read_csv(Path+'train.csv')\nprint('train reading complete')\ntest_rawdf=pd.read_csv(Path+'test.csv')\nprint('test reading complete')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"440ee2cee0e6d0f16ee2e44dd24a054d4be9ca2f"},"cell_type":"code","source":"cols_to_convert = ['product','gender']\nfor col in cols_to_convert:\n    print('col=',col)\n    train_rawdf[col], indexer = pd.factorize(train_rawdf[col])\n    test_rawdf[col] = indexer.get_indexer(test_rawdf[col])\n    if col in log_df.columns:\n        log_df[col] = indexer.get_indexer(log_df[col])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e3ef0d97c2f2fa6c57f469c3fc2927d9bdc7f877"},"cell_type":"code","source":"# log_df.to_hdf('log.hdf',key='data')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ef1992c6d5361c952b30e8c035656a2d6eba9862"},"cell_type":"code","source":"print(train_df.shape)\ntrain_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a750a612af2fbb94e0ee78ef013af319e591bebb"},"cell_type":"code","source":"print(test_df.shape)\nprint(100 *test_df.shape[0] / train_df.shape[0])\ntest_df.head()\n#train to test ratio\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a6e7ec478723ee2bf3db1f3857521a3a5b58534a"},"cell_type":"code","source":"# print(log_df.shape)\n# log_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6130bab8a3238619ef361b169bf031827ad7422a"},"cell_type":"code","source":"targetcol='is_click'\n#class balancing ratio\nnoof0s = train_df[train_df[targetcol]==0].shape[0]\nnoof1s = train_df[train_df[targetcol]==1].shape[0]\nprint('no of 0s:',noof0s)\nprint('no of 1s:',noof1s)\nratio0s = noof0s / (noof1s +noof0s )\nratio1s = noof1s / (noof1s +noof0s )\nprint('ratio of 0s:',ratio0s)\nprint('ratio of 1s:',ratio1s)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"292e37f1393e242d2fe0f32e4418aad5fa73123d"},"cell_type":"code","source":"train_df[targetcol] = train_df[targetcol].astype('int32')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"167fc0365719d4b7b2fc5a9a5d1c357f244c2c8b"},"cell_type":"code","source":"def get_opt_cutoff_prec(labels,preds):\n    precision, recall, thresholds  = precision_recall_curve(labels, preds)\n    f1_score= 2*((precision*recall)/(precision+recall))\n    optimal_idx = np.nanargmax(f1_score)\n    optimal_threshold = thresholds[optimal_idx]\n    return optimal_threshold, f1_score[optimal_idx]\n\ndef convert_probtolabels(preds,cutoff=0.5):\n    y_bin= preds.copy()\n    y_bin[preds>cutoff] = 1\n    y_bin[preds<=cutoff] = 0\n    y_bin=y_bin.astype(int)\n\n    return y_bin","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"28f995f7c0962b5e2a805ab9c691d713e90bd1a8"},"cell_type":"code","source":"cat_cols=['product','gender','user_id',  'campaign_id', 'webpage_id',\n'product_category_1','product_category_2','user_group_id','var_1']\nother_cols = ['age_level','user_depth','city_development_index']\ndate_feats = ['dayofweek','month','hour']\n\nraw_cols = cat_cols + other_cols + date_feats","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4921d7b570f3090e3c8831fffba761b9b781b578"},"cell_type":"code","source":"params = {\n    \"objective\" : \"binary\",\n    \"metric\" : \"auc\", \n    'colsample_bytree': 0.9779854557917957,\n   'learning_rate': 0.09313896377798163,\n   'min_child_samples': 40,\n   'num_leaves': 127,\n   'reg_alpha': 0.8606375999411122,\n   'reg_lambda': 0.27709140233989277,\n   'subsample': 0.5847676636997171,\n   'subsample_for_bin': 220000,\n   'bagging_seed': 2018,\n   'bagging_frequency': 1,\n    'n_estimators': 1000,\n    'random_state':1,\n    \"num_threads\": 4\n}\n\n# params = {\n#     \"objective\" : \"binary\",\n#     \"metric\" : \"auc\", \n#     \"num_leaves\" : 50, # 100\n#     \"min_child_samples\" : 150, # 50\n#     \"learning_rate\" : 0.02,\n#     \"bagging_fraction\" : 0.75,\n#     \"feature_fraction\" : 0.7, # 0.7,0.65\n#     \"bagging_frequency\" : 1,\n#     \"bagging_seed\" : 2018,\n#     \"verbosity\" : -1,\n#      \"lambda_l1\": 0.1,\n#     \"lambda_l2\": 0.7,\n#     'n_estimators': 1000,\n#     'random_state':1,\n#         \"num_threads\": 4\n# }","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9a2c99e4f5a4f80c7183611bc5a73294e2efd871"},"cell_type":"code","source":"# folds = get_folds(df=train_df, n_splits=5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a7af61840be6b23eb2ced97de94217f717285ba8"},"cell_type":"code","source":"targetcol='is_click'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d59203a3198a5bee24ab051fd83e86b8d437bc0e"},"cell_type":"code","source":"y_reg = (train_df[targetcol]==1).astype('int32')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"84f014de5554c707a5a04f22d6650ebd3a070aa5"},"cell_type":"code","source":"dates = train_df['DateTime']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0aaac1dcdad535e464f40016c101ccb21ca82a16"},"cell_type":"code","source":"# #Date Validation split\n# import datetime\n# #Take 5th July 2017 as valid start and 6th july as valid end\n# date_valid_start = datetime.date(2017, 7, 4) \n# date_valid_end = datetime.date(2017, 7, 5) \n\n# dates_train_filter = (dates <  date_valid_start) \n# dates_valid_filter = (dates >=  date_valid_start) & (dates <=  date_valid_end) \n\n# dates_train = dates[dates_train_filter]\n# dates_valid  = dates[dates_valid_filter]\n\n# train_df_valid = train_df.loc[dates_valid_filter]\n# y_reg_valid = (train_df_valid[targetcol]==1).astype('int32')\n\n# train_df_train = train_df.loc[dates_train_filter]\n# y_reg_train = (train_df_train[targetcol]==1).astype('int32')\n\n# print(train_df_train.shape)\n# print(train_df_valid.shape)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"72215d9c015521a01cc7fbcc3cbacc59a0f3d7e6"},"cell_type":"markdown","source":"**    Lag Feature Generation**"},{"metadata":{"trusted":true,"_uuid":"6025d6e4932246e0c9db59f82c854b62bea2349f"},"cell_type":"code","source":"# from tqdm import tqdm\n# import timeit\n\n# def createlagcol(val_shift,tolag_cols,join_cols,\n#                        data_shift,data_shift_join,\n#                        data):\n#     dateblocknumcol = 'day'\n#     data_shift[dateblocknumcol] = data_shift[dateblocknumcol] + val_shift\n\n#     lagcolumns = lambda x: '{}_lag_{}'.format(x, val_shift) if x in tolag_cols else x\n#     data_shift = data_shift.rename(columns=lagcolumns)\n    \n#     print('val_shift=',val_shift)\n#     print(data_shift.columns)\n   \n#     # Test shift would get the lags from train shift those exactly matches with the test set month.\n#     # Also exclude date_block_num column in test shift\n# #     colsfortest = list(train_shift.columns.difference([dateblocknumcol])) \n# #    print(\"colsfortest=\",colsfortest)\n# #     test_shift =  train_shift.loc[train_shift[dateblocknumcol]==(last_block+1),colsfortest]\n#     #Don't use the shifted date block num beyond maximum block no for train set\n#     data_shift = data_shift[data_shift[dateblocknumcol]<=last_block]\n#     print('shift.columns=',data_shift.columns)\n#     data_shift.set_index(join_cols,inplace=True)\n   \n#     start = timeit.default_timer()\n#     data_shift_join= data_shift_join.join(data_shift,on=join_cols,how='left')\n#     stop = timeit.default_timer()\n    \n#     print(\"Join complete: Execution Time={}\".format(stop - start ))\n   \n#     del data_shift\n# #     del test_shift\n#     gc.collect()\n    \n#     start = timeit.default_timer()\n#     lagcolumn_names = ['{}_lag_{}'.format(x, val_shift) for x in tolag_cols ]\n#     print(lagcolumn_names)\n#     for colname in lagcolumn_names:\n#         data[colname] = data_shift_join[colname]\n# #         test_data[colname]  = test_shift_join[colname].copy()\n#         del data_shift_join[colname]\n# #         del test_shift_join[colname]\n#         gc.collect()\n    \n#     stop = timeit.default_timer()\n#     print(\"Train and Test column copy execution time: {}\".format(stop - start ))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"66ec1cd164af0a94d635729b5b8145f525e640e2"},"cell_type":"code","source":"def gen_datefeats(df):\n    df['DateTime'] = pd.to_datetime(df['DateTime'],infer_datetime_format=True)\n    \n    df['day'] = df['DateTime'].dt.day\n    df['dayofweek'] = df['DateTime'].dt.dayofweek\n    df['month'] = df['DateTime'].dt.month\n    df['hour'] = df['DateTime'].dt.hour\n    return df\n\nlog_df=gen_datefeats(log_df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"fa72e50be599fd9bcf75904bdf0d3c353a3f56f4"},"cell_type":"code","source":"date_log_start = datetime.date(2017, 5, 28) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3ec0cce638d88945ce607cf63f4005e5df967131"},"cell_type":"code","source":"def gen_noofdayssincelog(df):\n    df['noofdays'] = (df['DateTime'].dt.date - date_log_start).dt.days\n    return df\nlog_df=gen_noofdayssincelog(log_df)\nprint('log df date generation complete')\ntrain_df=gen_noofdayssincelog(train_df)\ntest_df=gen_noofdayssincelog(test_df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b5f599875925ef8b3e2b09c8703587384480f43f"},"cell_type":"code","source":"def createlagcol(val_shift,tolag_cols,join_cols,\n                       data_shift,\n                       data):\n    dateblocknumcol = 'noofdays'\n    data_shift_proc = data_shift.copy()\n    data_shift_proc[dateblocknumcol] = data_shift[dateblocknumcol] + val_shift\n#     lagcolumns = lambda x: '{}_lag_{}'.format(x, val_shift) if x in tolag_cols else x\n#     data_shift_proc = data_shift_proc.rename(columns=lagcolumns)\n    print('bef datashift shape:',data_shift_proc.shape)\n    data_shift_proc = data_shift_proc[data_shift_proc[dateblocknumcol]<=last_block]\n    curcols = list(data_shift_proc.columns)\n#     data_shift_proc.columns[len(tolag_cols)-1] = \n    lag_colnames = [col+'_lag_'+str(val_shift) for col in tolag_cols]\n    data_shift_proc.columns= curcols[0:len(curcols)-1] + lag_colnames\n#     data_shift_proc.columns = [col+'_lag_'+ str(val_shift) if col in tolag_cols else col for col in data_shift_proc.columns]\n    print(data_shift_proc.columns)\n#     data_shift = data_shift.groupby(join_cols)[lag_colnames].agg('count')\n    \n    print('val_shift=',val_shift)\n    print('aft datashift shape:',data_shift_proc.shape)\n    \n    start = time.time()\n    data= pd.merge(data,data_shift_proc, how='left',on=join_cols)\n#     data= pd.merge(data,data_shift_proc, right_index=True, how='left')\n#     data_shift.set_index(join_cols,inplace=True)\n#     data= data.join(data_shift, how='left',on=join_cols)\n    stop = time.time()\n    \n    print(\"Join complete: Execution Time={}\".format(stop - start ))\n    \n    del data_shift_proc\n    gc.collect()\n    return data\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5629c328cb936c493744a6eb6f1c90086e7c3b70"},"cell_type":"code","source":"train_df['istrain'] = 1\ntest_df['istrain'] = 0\ntest_df[targetcol] = np.nan\ncombined_data = pd.concat([train_df,test_df],axis=0)\nprint(combined_data.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5f4e6aa3cd9d771b731bc9dd8f9aa67b5345988f"},"cell_type":"code","source":"# LAG TARGET COL\nfrom tqdm import tqdm\nimport timeit\n\nlast_block = 43\nindex_cols = ['user_id','product','noofdays']\n# List of columns that we will use to create lags\ncols_to_rename = [targetcol] \n\n\n# data_to_shift_columns = combined_data[index_cols + cols_to_rename].copy()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5f4e6aa3cd9d771b731bc9dd8f9aa67b5345988f"},"cell_type":"code","source":"start = time.time()\n# data_shift = combined_data[index_cols + cols_to_rename].groupby(index_cols)[cols_to_rename].agg('sum')\ndata_shift = combined_data.groupby(index_cols)[cols_to_rename].agg('sum')\nstop = time.time()\nprint(\"Group By complete: Execution Time={}\".format(stop - start ))\ntargetsumcolumns = lambda x: '{}_sum'.format(x) if x in cols_to_rename else x\ndata_shift = data_shift.rename(columns=targetsumcolumns)\nstart = time.time()\ndata_shift.reset_index(inplace=True)\nstop = time.time()\nprint(\"Index complete: Execution Time={}\".format(stop - start ))\n\ndata_shift = data_shift[~data_shift['is_click_sum'].isnull()]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c3b4d69bc510e396d941ec23e75533f3d4ca8143"},"cell_type":"code","source":"#groupby on log data\nlog_df[targetcol] = 1\nstart = time.time()\nlog_shift = log_df.groupby(index_cols)[cols_to_rename].agg('sum')\nstop = time.time()\nprint(\"Group By complete: Execution Time={}\".format(stop - start ))\ntargetsumcolumns = lambda x: '{}_sum'.format(x) if x in cols_to_rename else x\nlog_shift = log_shift.rename(columns=targetsumcolumns)\n\nstart = time.time()\nlog_shift.reset_index(inplace=True)\nstop = time.time()\nprint(\"Index complete: Execution Time={}\".format(stop - start ))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"014fb159fc9bd65ed72589ea0abd4578060ee077"},"cell_type":"code","source":"log_plus_data_shift = pd.concat([log_shift,data_shift],axis=0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false,"_uuid":"c186cba2d799ece57445e863118f8af36d34a4e6"},"cell_type":"code","source":"# data_shift_join=combined_data[index_cols].copy()\n\n# test_index_cols = index_cols.copy()\n# test_index_cols.remove(dateblocknumcol)\n# test_shift_join=test_df[test_index_cols].copy()\n\nshift_range = [2,3,4,5,7,14,21,30,31]\n\nfor val_shift in tqdm(shift_range):\n#     data_shift = data_to_shift_columns.copy()\n    combined_data = createlagcol(val_shift,cols_to_rename,index_cols,\n                       log_plus_data_shift,\n                       combined_data) \n    print('combined_data shape:',combined_data.shape)\n\n            \n# del data_shift_join\n# del test_shift_join\n# del data_to_shift_columns\ngc.collect()\n# colstodisplay = index_cols\n# for val_shift in tqdm(shift_range):\n#     colstodisplay.append('target'+'_lag_'+str(val_shift))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a42b9d7f3c4ea2cce21a4897ea7e31ecbe669e05"},"cell_type":"code","source":"train_df = combined_data[combined_data['istrain']==1]\ntest_df = combined_data[combined_data['istrain']==0]\ndel test_df['istrain']\ndel test_df[targetcol]\ndel train_df['istrain']\ndel combined_data;gc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"29d4664cb664127e5e423d7bd71c01540f0b1dd8"},"cell_type":"code","source":"#Set category field types for Model classification\ntemp=pd.DataFrame()\nfor col in cat_cols:\n    train_df[col] =train_df[col].astype('category')\n    test_df[col] =test_df[col].astype('category')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9a2c99e4f5a4f80c7183611bc5a73294e2efd871"},"cell_type":"code","source":"#train features\n# train_features = [_f for _f in train_df.columns if _f not in excluded_features]\n# enc_cols = [col for col in train_df.columns if ('targetenc' in col) ]\nenc_cols_excl_action = [col for col in train_df.columns if ('targetenc' in col)\n                       and ('action_' not in col)\n                       ]\n# raw_action_cols = [col for col in train_df.columns if ('targetenc' not in col)\n#                        and ('action_' in col)\n# #                        and ('date_mean_total' not in col)\n# #                          and ('_viewsum' not in col)\n#     ]\nderived_raw_action_cols= [col for col in train_df.columns \n                         if ('targetenc' not in col) and (('date_mean_total' in col) ) ]\nkey_raw_action_cols = ['action_mean', 'action_sum', 'action_count', 'action_month_mean', 'action_month_sum', \n                       'action_month_count', 'action_dayofweek_mean', 'action_dayofweek_sum', \n                       'action_dayofweek_count', 'action_hour_mean', 'action_hour_sum', 'action_hour_count']                  \nkey_cols =  ['user_id','product']\ntarget_enc_keyactioncols =['targetenc_action_sum']\nlagcols = [col for col in train_df.columns if '_lag_' in col ]\ntrain_features = raw_cols + key_raw_action_cols + derived_raw_action_cols + enc_cols_excl_action + lagcols\n\nsummarize_enc_cols =['sum_targetenc','product_targetenc','mean_targetenc']\nexcluded_features = []\ntrain_features = [_f for _f in train_features if _f not in excluded_features]\n\n# train_features =['user_id', 'product', 'campaign_id', 'webpage_id', 'product_category_1', 'product_category_2', 'user_group_id', 'gender', 'age_level', 'user_depth', 'city_development_index', 'var_1', 'action_mean', 'action_sum', 'action_count', 'day', 'dayofweek', 'month', 'hour']\n\nprint(train_features)\nprint(len(train_features))\n\nimportances = pd.DataFrame()\noof_cls_preds = np.zeros(train_df.shape[0])\nsub_cls_preds = np.zeros(test_df.shape[0])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c5c249963211f12a0ffc900ca77b2d10353666d0"},"cell_type":"code","source":"#Date Validation split\nimport datetime\n#Take 6th July 2017 as valid start date and 7th july as valid end\ndate_valid_start = datetime.date(2017, 7, 6) \n\ndates_valid_filter = (dates >=  date_valid_start)\n\ndates_train = dates[~dates_valid_filter]\ndates_valid  = dates[dates_valid_filter]\n\ntrain_df_valid = train_df.loc[dates_valid_filter]\ny_reg_valid = (train_df_valid[targetcol]==1).astype('int32')\n\ntrain_df_train = train_df.loc[~dates_valid_filter]\ny_reg_train = (train_df_train[targetcol]==1).astype('int32')\n\nprint(train_df_train.shape)\nprint(train_df_valid.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e5a096b1a11b13f5a58b676340c0e5288b7618e4"},"cell_type":"code","source":"start = time.time()\n\n# iters = len(folds)\nval_aucscores=[]\nimportances['feature'] = train_features\nimportances['gain'] = 0\n\nreg = lgb.LGBMClassifier(**params)\n\nreg.fit(\n    train_df_train[train_features], y_reg_train,\n    eval_set=[(train_df_valid[train_features], y_reg_valid)],\n    early_stopping_rounds=50,\n    verbose=50,\n    eval_metric='auc' \n    )\n    \nimportances['gain'] = reg.booster_.feature_importance(importance_type='gain') \n\nvalid_preds = reg.predict_proba(train_df_valid[train_features], num_iteration=reg.best_iteration_)[:,1]\nauc_score = roc_auc_score(y_reg_valid,valid_preds)\nprint('valid auc score=',auc_score)\n# _preds = reg.predict_proba(test_df[train_features], num_iteration=reg.best_iteration_)[:,1]\n# sub_cls_preds += _preds / iters\nval_iterations = reg.best_iteration_\nprint('valid iterations =',val_iterations)\nend =  time.time()\nprint('LGB Execution Time:',end-start)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d384954343e3a533837fc106e9c86eca51601ffc"},"cell_type":"code","source":"#Model FUll Train Training\nstart = time.time()\n\nimportances['feature'] = train_features\nimportances['gain'] = 0\n\n#validation iteration returned 72 iterations and test predictions for nearby iterations\nest_list =[90,100,110,120,130]\n\nfor est in est_list:\n    params['n_estimators'] = est\n    reg = lgb.LGBMClassifier(**params)\n\n    print('Light GBM Fit Start..')\n    reg.fit(\n        train_df[train_features], y_reg,\n        eval_metric='auc' \n        )\n\n    importances['gain'] = reg.booster_.feature_importance(importance_type='gain') \n\n    print('Light GBM Test Pred Start..')\n    sub_cls_preds = reg.predict_proba(test_df[train_features], num_iteration=reg.best_iteration_)[:,1]\n    end =  time.time()\n    print('LGB Execution Time:',end-start)\n    #Save Test Submission File\n    sub_df = pd.DataFrame({\"session_id\":test_df['session_id']})\n    sub_df[targetcol] = sub_cls_preds\n    sub_df.to_csv(\"timeseriesbaseline_\" + str(est) + \".csv\", index=False)\n    print('Submission Save Complete ')\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c47803a736a739d2981837fc922dcac4a927c9f7","scrolled":false},"cell_type":"code","source":"# #Train Score    \n# print('oof auc=',roc_auc_score(y_reg,oof_cls_preds))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"88e8a41b4762d5cf0495ab7eb73aef81dbc6686a"},"cell_type":"code","source":"# #Save Initial Train Meta\n# np.savetxt('LGBM_TrainMeta.npy',oof_cls_preds)\n# np.savetxt('LGBM_TestMeta.npy',sub_cls_preds)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2a4ed6b8d44e5b28e6a94b3a79e94999ca29ac92","scrolled":true},"cell_type":"code","source":"# vis_importances = importances[importances['fold']==2]\nimportances.sort_values(by='gain', ascending=False,inplace=True)\n# vis_importances1= vis_importances1[vis_importances1['gain']==0]\n# vis_importances1.sort_values(by='feature', ascending=False,inplace=True)\nimportances[0:100]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7928db8b389da5269ecdd4229c32535e70e0c8d5"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}